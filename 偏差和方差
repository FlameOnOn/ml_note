假设你的算法在开发集上有 16% 的错误率（84% 精度），我们将这 16% 的错误
率分为两部分：
• 第一部分是算法在训练集上的错误率。在本例中，它是 15%。我们非正式地将它作为算法
的偏差（bias）
• 第二部分指的是算法在开发集（或测试集）上的表现比训练集上差多少。在本例中，开发集 表现比训练集差 1%。我们非正式地将它作为算法的方差（variance）。

在统计学领域有着更多关于偏差和方差的正式定义，但不必担心。粗略地说，偏差指的是算法在大型 训练集上的错误率；方差指的是算法在开发集(或测试集)上的表现低于训练集的程度。
当你使用均方误差（MSE）作为误差度量指标时，你可以写下偏差和方差对应的两个公式，并且证明总误差=偏差+方差。但在 处理机器学习问题时，此处给出的偏差和方差的非正式定义已经足够。

在训练集上误差低,在开发集上误差高.说明泛化能力低,也就是过拟合
在训练集上误差高,在开发集上误差也高,而且两者差不多,说明偏差大,也就是欠拟合
在训练集上误差高,在开发集上比训练集还要高,那偏差也大,方差也大. 过拟合和欠拟合的名词不太好应用于此

假如一个语音系统在开发集上的错误率是30%,在训练集上的错误率是15%,最优错误率是14%(世界上最好的语音系统也就是14%的错误率了.可以和人工对比得出这个最优错误率,或者干脆自己定一个,符合这个错误率业务上就
够用了,在统计学上，最优错误率也被称为贝叶斯错误率（Bayes error rate），或贝叶斯率)

可以将 30% 的总开发集误差分解如下（类似的分析可以应用于测试集误差）:
● 最优错误率（“不可避免偏差”）：14%。假设我们决定，即使是世界上最好的语音系统，仍会有 14% 的误差。我们可以将其认为是学习算法的偏差“不可避免”的部分。
● 可避免偏差：1%。即训练错误率和最优误差率之间的差值。(如果可避免偏差值是负的，即算法在训练集上的表现比最优错误率要好。这意味着你正在过拟合训练集，并且 算法已经过度记忆（over-memorized）训练集。你应该专注于有效降低方差的方法，而不是选择进一步减少偏差 的方法)
● 方差：15%。即开发错误和训练错误之间的差值。

偏差 = 最佳误差率（“不可避免偏差”）+ 可避免的偏差
这个“可避免偏差”反映了算法在训练集上的表现比起“最优分类器”差多少。
方差的概念和之前保持一致。理论上来说，我们可以通过训练一个大规模训练集将方差减少到
接近零。因此只要拥有足够大的数据集，所有的方差都是可以“避免的”，所以不存在所谓的“不
可避免方差”。

下面是处理偏差和方差问题最简单的形式：
● 如果具有较高的可避免偏差，那么加大模型的规模（例如通过添加层/神经元数量来增加神经网络的大小）,如果你的算法含有一个精心设计的正则化方法，通常可以安全地加大模型的规模，而不用担心增加过拟合风险.
假设你正在应用深度学习方法，使用了 L2 正则化和 dropout 技术，并且设置了在开发集上表现最好的正则化参数。此时你加大模型规模，算法的表现往往会保持不变或提升；它不太可能
明显地变差。这种情况下，不使用更大模型的唯一原因就是这将使得计算代价变大。
● 如果具有较高的方差，那么增加训练集的数据量。

你可能听过“偏差和方差间的权衡”。目前，在大部分针对学习算法的改进中，有一些能够减少
偏差，但代价是增大方差，反之亦然。于是在偏差和方差之间就产生了“权衡”。
例如，加大模型的规模（在神经网络中增加神经元/层，或增加输入特征），通常可以减少偏
差，但可能会增加方差。另外，加入正则化一般会增加偏差，但能减少方差。
在现代，我们往往能够获取充足的数据，并且可以使用非常大的神经网络（深度学习）。因此
，这种权衡的情况比较少，并且现在有更多的选择可以在不损害方差的情况下减少偏差，反之
亦然。
例如，一般情况下，你可以通过增加神经网络的规模大小，并调整正则化方法去减少偏差，而
不会明显的增加方差。通过增加训练数据，你也可以在不影响偏差的情况下减少方差。
如果你选择了一个非常契合任务的模型架构，那么你也可以同时减少偏差和方差。只是选择这
样的架构可能有点难度。
