如果你的学习算法存在着很高的可避免偏差，你可能会尝试以下方法：
● 加大模型规模（例如神经元/层的数量）：这项技术能够使算法更好地拟合训练集，从
而减少偏差。当你发现这样做会增大方差时，通过加入正则化可以抵消方差的增加。
● 根据误差分析结果修改输入特征：假设误差分析结果鼓励你增加额外的特征，从而帮助
算法消除某个特定类别的误差。（我们会在接下来的章节深入讨论这个话题。）这些新
的特征对处理偏差和方差都有所帮助。理论上，添加更多的特征将增大方差；当这种情
况发生时，你可以加入正则化来抵消方差的增加。
● 减少或者去除正则化（L2 正则化，L1 正则化，dropout）：这将减少可避免偏差，但
会增大方差。
● 修改模型架构（比如神经网络架构）使之更适用于你的问题：这将同时影响偏差和方
差。
有一种方法并不能奏效：
● 添加更多的训练数据：这项技术可以帮助解决方差问题，但它对于偏差通常没有明显的
影响。

如果你的学习算法存在着高方差问题，可以考虑尝试下面的技术： ● 添加更多的训练数据：这是最简单最可靠的一种处理方差的策略，只要你有大量的数据
和对应的计算能力来处理他们。 ● 加入正则化（L2 正则化，L1 正则化，dropout）：这项技术可以降低方差，但却增大
了偏差。
● 加入提前终止（例如根据开发集误差提前终止梯度下降）：这项技术可以降低方差但却
增大了偏差。提前终止（Early stopping）有点像正则化理论，一些学者认为它是正则
化技术之一。
● 通过特征选择减少输入特征的数量和种类​：这种技术或许有助于解决方差问题，但也可
能增加偏差。稍微减少特征的数量（比如从 1000 个特征减少到 900 个）也许不会对
偏差产生很大的影响，但显著地减少它们（比如从 1000 个特征减少到 100 个，10 倍
地降低）则很有可能产生很大的影响，你也许排除了太多有用的特征。在现代深度学习
研究过程中，当数据充足时，特征选择的比重需要做些调整，现在我们更可能将拥有的
所有特征提供给算法，并让算法根据数据来确定哪些特征可以使用。而当你的训练集很
小的时候，特征选择是非常有用的。
● 减小模型规模（比如神经元/层的数量）：谨慎使用。这种技术可以减少方差，同时可
能增加偏差。然而我不推荐这种处理方差的方法，添加正则化通常能更好的提升分类性
能。 减少模型规模的好处是降低了计算成本，从而加快了你训练模型的速度。如果加
速模型训练是有用的，那么无论如何都要考虑减少模型的规模。但如果你的目标是减少
方差，且不关心计算成本，那么考虑添加正则化会更好。
下面是两种额外的策略，和解决偏差问题章节所提到的方法重复：
● 根据误差分析结果修改输入特征：假设误差分析的结果鼓励你创建额外的特征，从而帮
助算法消除某个特定类别的误差。这些新的特征对处理偏差和方差都有所帮助。理论上
，添加更多的特征将增大方差；当这种情况发生时，加入正则化，这可以消除方差的增
加。
● 修改模型架构（比如神经网络架构）使之更适用于你的问题：这项策略将同时影响偏差
和方差。
