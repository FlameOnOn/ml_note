特征选择技术可以精简掉无用的特征，以降低最终模型的复杂性，它的最终目的是得到一个简约模型，在不降低预测准确率或对预测准确率影响不大的情况下提高计算速度(也可以提升准确率)。为了得到这样的模型，
有些特征选择技术需要训练不止一个待选模型。换言之，特征选择不是为了减少训练时间（实际上，一些技术会增加总体训练时间），而是为了减少模型评分时间。
也就是说,模型训练本身的时间再加上减少特征训练模型的时间,超过了之前用所有特征训练模型的时间.所以总体训练时间不会减少. 其实,特征选择可能就执行一次,之后定时去训练模型的时候,特征少还是会比之前的
训练时间少的.
如果有一些特征是无用的,或者错误的干扰特征,把这些特征去掉反而会提升模型的准确率.
模型评分时间,可以理解为在线预测或者分类时间. 特征少了,模型简约了,肯定会减少线上预估的时间的.

特征选择技术对于处理大量交互特征是必需的
粗略地说，特征选择技术可以分为以下三类。 (过滤技术是不经过模型的,其他两个都是经过模型的)

过滤
过滤技术对特征进行预处理，以除去那些不太可能对模型有用处的特征。例如，我们可
以计算出每个特征与响应变量之间的相关性或互信息，然后过滤掉那些在某个阈值之下
的特征。第 3 章将讨论用于文本特征的这种技术。过滤技术的成本比下面描述的打包技
术低廉得多，但它们没有考虑我们要使用的模型，因此，它们有可能无法为模型选择出
正确的特征。我们最好谨慎地使用预过滤技术，以免在有用特征进入到模型训练阶段之
前不经意地将其删除。

打包方法
这些技术的成本非常高昂，但它们可以试验特征的各个子集，这意味着我们不会意外地
删除那些本身不提供什么信息但和其他特征组合起来却非常有用的特征。打包方法将模
型视为一个能对推荐的特征子集给出合理评分的黑盒子。它们使用另外一种方法迭代地
对特征子集进行优化。

嵌入式方法
这种方法将特征选择作为模型训练过程的一部分。例如，特征选择是决策树与生俱来的
一种功能，因为它在每个训练阶段都要选择一个特征来对树进行分割。另一个例子是
ℓ1 正则项，它可以添加到任意线性模型的训练目标中。ℓ1 正则项鼓励模型使用更少的特
征，而不是更多的特征，所以又称为模型的稀疏性约束。嵌入式方法将特征选择整合为
模型训练过程的一部分。它们不如打包方法强大，但成本也远不如打包方法那么高。与
过滤技术相比，嵌入式方法可以选择出特别适合某种模型的特征。从这个意义上说，嵌
入式方法在计算成本和结果质量之间实现了某种平衡。
特征选择的完整操作方法超出了本书范围，对此感兴趣的读者可以参考 Guyon 和 Elisseeff在 2003 年做的文献综述。

