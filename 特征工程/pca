pca
PCA 的核心思想是，使用一些新特征代替冗余特征，这些新特征能恰当地总结初始特征空
间中包含的信息

要想理解线性代数公式，必须搞清楚哪些量是标量，哪些量是向量，以及向
量的方向——垂直的还是水平的。要知道矩阵的维度，因为它们经常会告诉
你需要的向量是在行中还是在列中。在一张纸上画出表示矩阵或向量的长方
形，确保它们的形状是匹配的。就像注意测量单位（比如用英里表示距离，
用英里每小时表示速度）可以让你深刻理解代数问题一样，在线性代数中，
你要注意的就是维度。

这个问题就变成了使新特征空间中数据点的方差最大化

关于 PCA 的讨论到此结束。关于 PCA 我们应该记住的主要两点是它的原理（线性投影）和
目标（最大化投影数据的方差）。PCA 的解要用到协方差矩阵的特征值分解，它与数据矩阵
的 SVD 联系非常紧密。还可以这样形象地理解 PCA：将数据挤入一张尽量松软的薄饼中。
PCA 是一种模型驱动的特征工程方法。（只要看到一个目标函数，就应该马上猜想它的背
后可能隐藏着一个模型。）其中的建模假设是方差能够很好地表示数据中包含的信息。同
样，模型要找出的是特征之间的线性相关性。在若干应用中可以使用 PCA 来减少相关性
或找出输入中常见的因子。
PCA 是一种广为人知的数据降维方法，但它也有自身的局限性，比如高昂的计算成本和不
能解释的结果。它适合应用在预处理阶段，特别是特征之间存在线性相关性的时候。
当作为一种消除线性相关性的方法时，PCA 还可以和白化方法结合使用。它的“表亲”ZCA
可以用可解释的方式对数据进行白化，但不能降低数据维度
