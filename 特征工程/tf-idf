tf-idf表示词频 - 逆文档频率。tf-idf 计算的不是数据集中每个单词在每个文档中的原本计数，而是一个归一化的计数,(所以它也属于特征工程的一种)其中每个单词的计
数要除以这个单词出现在其中的文档数量。即：
bow(w, d) = 单词 w 在文档 d 中出现的次数
tf-idf(w, d) = bow(w, d) * N / ( 单词 w 出现在其中的文档数量 )

 N是数据集中的文档总数。分数 N / ( 单词 w 出现在其中的文档的数量 ) 就是所谓的逆文档
频率。如果一个单词出现在很多文档中，那么它的逆文档频率就接近于 1。如果一个单词
只出现在少数几个文档中，那么它的逆文档频率就会高得多。

我们也可以使用逆文档频率的对数变换，而不是它的原始形式。对数变换可以将 1 转换为
0，并使大的数值（那些远远大于 1 的值）变小。（随后会有更多介绍。）
如果将 tf-idf 定义为：
tf-idf(w, d) = bow(w, d) * log(N / 单词 w 出现在其中的文档数量 ) 那么就可以有效地将一个几乎出现在所有单个文档中的单词的计数归零，而一个只出现在
少数几个文档中的单词的计数将会被放大
通过词袋表示，与特征数量相比，列空间相对较小。可能有些单词在同一文档中出现了大致相同的次数，这会导致相应的列向量几乎是线性相关的，进而导致列空间不是满秩的这种情况称为秩亏。
秩亏的行空间和列空间会使模型对问题用力过猛.如果行空间和列空间是满秩的,模型会生成输出空间中的任何目标向量。
当它们秩亏时，模型会具有更多不必要的自由度，这会导致更加难以确定最终解
特征缩放可以解决数据矩阵的秩亏问题吗?
列空间定义为所有列向量的线性组合,特征缩放使用向量与一个常数的乘积来代替这个向量,征缩放不会改变列空间的秩。同样，特
征缩放也不会影响零空间的秩
但是，这里仍然有一个问题。如果相乘的标量为 0，那么就无法恢复初始的线性组合，v1
就消失了。如果这个向量与其他列线性无关，那么我们就有效地缩减了列空间并扩展了零
空间。
如果这个向量与目标输出不相关，那么我们就有效地消除了噪声信号，这是非常好的事
情。这就是 tf-idf 与 ℓ2 归一化之间的关键区别。ℓ2 归一化永远不会计算出一个值为 0 的范
数，除非向量中都是 0。如果向量接近于 0，那么它的范数也接近于 0。除以一个小范数会
突出这个向量，并使它更长.
另一方面，tf-idf 会生成一个接近于 0 的缩放因子,当单词出现在训练集中
的大量文档中时，会出现这种情况，这种单词很可能与目标向量没有很强的相关性。除去
这种单词，可以使解决方案更关注列空间中的其他方向，并找到更好的解（然而准确率的
提高幅度很可能不会很大，因为使用这种方法通常找不到太多能削减的噪声方向）

下面这段话不太明白:
特征缩放（包括 ℓ2 归一化和 tf-idf）的真正用武之地是加快解的收敛速度。这表现在它能
使数据矩阵具有明显更少的条件数（最大奇异值和最小奇异值的比值，参见附录 A 中关于
这些名词的详细讨论）。实际上，ℓ2 归一化使得条件数几乎为 1。但并不是条件数越少，解
就越好。在这次实验中，ℓ2 归一化收敛得比词袋和 tf-idf 都快得多，但它对过拟合更加敏
感：它需要更多的正则化，而且对优化过程中的迭代次数更加敏感


与普通的词袋表示相比，tf-idf 和 ℓ2 归一化并没有提高最终分类器的
准确率。经过一些统计建模和线性代数分析，我们意识到了原因：它们都没有改变数据矩
阵的列空间。
二者之间有个小区别，那就是 tf-idf 既可以“拉长”单词计数，也可以“压缩”它。换句
话说，它可以使某些计数变大，同时使其他计数接近于 0。因此，tf-idf 可以比较彻底地消
除那些没有信息量的单词。
通过这种方法，我们还发现了特征缩放的另一个作用：它可以减少数据矩阵的条件数，大
大加快训练线性模型的速度。ℓ2 归一化和 tf-idf 都有这种效果。
总而言之，我们的收获是：正确的特征缩放有助于分类问题。正确缩放可以突出有信息量
的单词，并削弱普通单词的影响。它还可以减少数据矩阵的条件数。正确的缩放不一定是
标准的列缩放。
